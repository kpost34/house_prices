---
title: "Machine Learning with Ames, IA, Housing Prices"
author: "Keith Post"
format: html
editor: visual
engine: knitr
---

```{r setup}

#load packages
library(pacman)
p_load(here, DT)

#source objects from scripts
source(here("code", "01_data-clean_check_impute.R"))
source(here("code", "02_feature-selection_engineering.R"))
source(here("code", "04_prep-test-data.R"))

#source modelling objects from files
model_cv_metrics_fp <- here("modelling", "mod_cv_metrics.rds")
grid_rf_fp <- here("modelling", "grid_rf.rds")
cv_metrics_tune_rf_fp <- here("modelling", "metrics_tune_rf.rds")
metric_tune_rf_plot_fp <- here("modelling", "metric_tune_rf_plot.rds")
final_model_fp <- here("modelling", "final_model.rds")
vi_final_model_fp <- here("modelling", "vi_final_model.rds")
vip_final_model_fp <- here("modelling", "vip_final_model.rds")
overall_qual_sale_price_plot_fp <- here("modelling", "overall_qual_sale_price_plot.rds")

tab_model_cv_metrics <- readRDS(model_cv_metrics_fp)
grid_rf <- readRDS(grid_rf_fp)
tab_cv_metrics_tune_rf <- readRDS(cv_metrics_tune_rf_fp)
fig_metric_tune_rf <- readRDS(metric_tune_rf_plot_fp)
final_fit <- readRDS(final_model_fp)
tab_vi_rf_final_model <- readRDS(vi_final_model_fp)
fig_vip_rf_final_model <- readRDS(vip_final_model_fp)
fig_overall_qual_sale_price_box <- readRDS(overall_qual_sale_price_plot_fp)

```

## Background

Given 79 explanatory variables describing houses in Ames, IA, the objective of this Kaggle competition is to develop a model to accurately predict home sale prices. A summary of the data can be found \[here\].\
(<https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data>)

The data are pre-split into training and test data sets. The former has a column with sale prices, while the latter does not.

## I. Initial Data Cleaning

The variables from the raw training data were categorized as character, double, or integer types when read into R. The following cleaning steps were taken prior to a deeper exploration of the data:\
\* column names were converted from snake to camel case\
\* *garage_yr_blt* was populated with -1000 if the value was not applicable and NA if missing\
\* not applicable values were populated with a variable-specific character (e.g., "NG" for "no garage", "NB" for "no basement") for multiple variables (e.g., *garage_type*, *garage_finish*, *bsmt_cond*, *bsmt_qual*)\
\* all character variables aside from *id* were converted to (unordered and ordered) factors; Table 1)

```{r tab1-variable classes}

DT::datatable(tab_var_class)

```

## II. Preliminary Data Checking

Data were checked by looking at the dimensions (\[inline R code\]) and summary information (Table 2a-c).

`::: {.panel-tabset}`

### Character types

```{r tab2a-chr summary}

DT::datatable(tab_train_chr)

```

Unsurprisingly, *id* is the only variable and has no missing values and all values are unique.

### Factor types

```{r tab2b-fct summary}

DT::datatable(tab_train_fct)

```

There are 46 factors--23 unordered and 23 ordered--and only four have missing values. Numbers of factor levels (classes/categories) range from 2-25.

### Numeric types

```{r tab2c-num summary}

DT::datatable(tab_train_num)

```

There are 34 numeric variables, including the target variable. Only two variables contain missing values. Many of the variables (e.g., *bsmt_fn_sf1*, *bsmt_half_bath*) contain high frequencies of 0s as indicated by the quartiles.

:::

## III. Missingness

### A. Assess missingness

Missingness of training data were first visualized.

```{r fig1-overall missing}

fig_train_overall_miss
```

Figure 1 shows that only 0.2% of data are missing, and that most of the missingness occurs in one variable.

Let's look more closely at variables with missing data.

```{r fig2-subset missing}

fig_train_sub_miss
```

Focusing on columns with missingness (Figure 2), *lot_frontage* has 18% missing values, *mas_vnr_type* and *mas_vnr_area* each have 1% missing values, and the remainder have less than 0.1% missing values.

Let's examine missingness by rows.

```{r fig3-missing by row}

fig_train_miss_row 
```

Figure 3 shows that by far most observations lack a missing value and for those that have at least one NA, it's overwhelmingly just one.

It's also important to note how missing values co-occur in the training data.

```{r fig4-missing pattern}

fig_train_miss_patt 
```

Figure 4 indicates that *mas_vnr_type* and *mas_vnr_area* have identical patterns of missingness.

Let's determine the specifics on number and percent missingness by predictor to better determine patterns of missingness.

```{r tab3-missingness by var}

DT::datatable(tab_train_miss)
```

There are only six predictors with missing values and as mentioned before *lot_frontage* has the greatest proportion of missing values with nearly 18% values absent (Table 3).

Let's evaluate the three predictors with the most missingness to determine whether missingness should be considered 'missing at random' (MAR) or 'missing completely at random' (MCAR).

`::: {.panel-tabset}`

#### *sale_price* by *lot_frontage* missing status

```{r fig5a-sale_price distribution by lot_frontage missing}

fig_lot_frontage_miss_sp
```

Although these distributions are similar, the plot where *lot_frontage* is missing is shifted slightly left (Figure 5a).

#### *lot_shape* by *lot_frontage* missing status

```{r fig5b-lot_shape distribution by lot_frontage missing}

fig_ls_dist_lt_miss

```

The frequency distributions of *lot_shape* by presence/absence of *lot_frontage* clearly differ (Figure 5b).

#### *foundation* by *mas_vnr_type* missing status

```{r fig5c-foundation by mas_vnr_type}

fig_foundation_dist_mvt

```

As indicated in Figure 4, *mas_vnr_type* and *mas_vnr_area* have the same pattern of missingness, so only one needs to be assessed. Here, the pattern of *foundation* (a related variable) frequencies differed when *mas_vnr_type* was present or absent.

:::

### B. Impute values

Given all of these results, the data do not appear to have MCAR missingness and thus should be considered MAR (note: MNAR would require deeper investigation and gathering more data, which is beyond the scope of this project). Multiple imputation is a reasonable approach for MAR data. The classification and regression tree ('cart') approach was used to impute missing values.

The pre- and post-imputation dataframes were visualized (Figure 6).

```{r fig6-pre-post imputation training}

fig_train_pre_post_impute
```

Unsurprisingly, the most frequent changes (i.e., NA to non-NA values) occurred within *lot_frontage* (Figure 6).

A closer look at *lot_frontage* shows the relationship between this predictor and *sale_price* with and without imputed values (Figure 7).

```{r fig7-lot_frontage before after imputation}

fig_train_lf_pre_post_impute
```

Figure 7 indicates similar relationships between *lot_frontage* and *sale_price* without or with imputed *lot_frontage* values.

## IV. Feature Selection

After a successful imputation of missing values, feature selection was conducted. Here factor variables that display constancy or low 'variance' and highly correlated variables were dropped.

### A. Drop Low 'Variance' (Frequency) Features

Low 'variance' factor variables means that one category composes at least 98% of the values. Thus, less than 2% of the remaining values for that factor are associated with one or more other categories. There's a couple reasons to drop these predictors:

1.  In v-fold cross-validation, there needs to be at least one value of each factor level in each fold. If infrequent factor levels are very rare, then this may not occur or the number of folds may need to be capped at a low number to accommodate this.
2.  Even if item 1 is accomplished, a small number of a given factor level in one or more folds may lead to over-fitting due to small sample size

Five factors exhibited constancy: *condition2*, *pool_qc*, *roof_matl*, *street*, *utilities* (Table 4).

```{r tab4-factor contancy}

DT::datatable(tab_train_fct_constancy)

```

### B. Remove Highly Correlated Features

Numerical predictors (i.e., integer, double-precision) were assessed for multicollinearity by calculating Pearson correlations for each pair of predictors.

```{r tab5-multicollinearity numerical predictors}

DT::datatable(tab_train_num_corr)

```

Table 5 shows that no pair of numerical predictors had a very strong correlation (\|*rI*\| \< 0.9 for all pairs). Note that this result was unaffected if Spearman rank correlations were used.

Multicollinearity was also assessed for pairs of ordered factors by computing Spearman rank correlations and using the same threshhold for exclusion.

```{r tab6-multicollinearity ordered factor predictors}

DT::datatable(tab_train_ord_corr)

```

Table 6 indicates that *overall_qual* and *overall_cond* are perfectly correlated, so *overall_cond* was removed. No other pairs of ordered factors had very strong correlations (\|*r*\| \> 0.9).

Multicollinearity among pairs of unordered factors was also investigated using Chi-square tests of independence.

```{r tab7-multicollinearity unordered factor predictors}

DT::datatable(tab_fct_x2_p %>%
                mutate(sig=p.value <= .05))

```

Table 7 shows that nearly all pairwise comparisons were significant at alpha = 0.05. Thus, the most significant pairs should be assessed further to determine which, if any, factors should be dropped.

After assessing the 10 chi-square test results with the smallest *p* values, the following comparisons seem to be the most indicative of collinearity among pairs of unordered factors.

```{r fig8-multicollinearity unordered factor pairs}

fig_fct_x2

```

Figure 8 illustrates strong patterns of interdependence among *ms_sub_class* and *bldg_type* as well as *exterior1st* and *exterior2nd*. A review of the 10 comparisons with the smallest *p*-values (Table 7) indicates that for these two pairs, *ms_sub_class* and *exterior2nd* should be dropped. The former is part of the three most significant correlations, and the latter is, by definition, secondary to *exterior1st*.

## V. Feature Engineering

### A. Rare-label Encoding

Although factors (both unordered and ordered) that exhibited constancy were dropped, many of the remaining ones still contained rare levels. In those cases, the rare levels should be combined together or with more common levels such that no level should be considered rare (i.e., composes \< 2% of observations). This feature engineering approach was performed for all applicable factors by 1) considering each level's relationship with *sale_price*, 2) combining levels by similarity of type, 3) using domain knowledge, and/or 4) simply combining rare levels into some type of 'Other' category.

#### Unordered Factors

One example of rare-label encoding involves *lot_config*.

```{r tab8-rare-label encoding lot_config}

DT::datatable(list_rare_cat_viz[["lot_config"]]$freq_table)

```

```{r fig9-rare-label encoding lot_config}

list_rare_cat_viz[["lot_config"]]$plots

```

Table 8 shows that category 'FR3' occurs less than 0.3% of the time, well below the 2% threshold. Given that 'FR3' is so rare, its relationship with *sale_price* is less important (which is indicative of the large error bars in the top two plots; Figure 9); thus, it was binned with 'FR2' (as these are similar in shape and frequency) to make 'FR2_3.'

Below is a summary of how unordered factor levels were combined.

```{r tab9-summary binning rare fct}

DT::datatable(tab_bin_fct)

```

#### Ordered Factors

Below is a summary of how ordered factor levels were combined.

```{r tab10-summary binning rare ord}

DT::datatable(tab_bin_ord)

```

### B. Feature Scaling

Feature scaling is performed on numerical predictors to normalize their ranges such that these predictors do not have a differential impact on the model simply as an artifact of differences in scales (ranges).

Normality of predictors is important to understand before deciding which scaling technique to employ. This was assessed visually using q-q plots.

::: panel-tabset
```{r fig10a-qqplot feature scale}

fig_qq_num1

```

```{r fig10b-qqplot feature scale}

fig_qq_num2

```

```{r fig10c-qqplot feature scale}

fig_qq_num3

```
:::

Many of the q-q plots (e.g., *bsmt_fin_sf1*, *garage_area*, *lot_area*) raise concerns about normal distributions (Figures 10a-c).

Normality was also assessed statistically with Shapiro tests.

```{r tab11-shapiro feature scale}

DT::datatable(tab_shapiro)

```

All numerical predictors were not normally distributed (*p* \< 0.05) per the results of the Shapiro tests (Table 11).

Thus, given both sets of evidence, normalization is an appropriate feature scaling technique for these predictors. This transformation was applied to the training data.

## VI. Cross-Validation and Modelling

Following initial data cleaning, imputation, feature scaling, and feature engineering, the training data underwent cross-validation.

### A. Create folds

The training data were split into 10 folds prior to model fitting.

### B. Model training data

Three different model types were used in modelling the house price data: decision tree, random forest, and K-nearest neighbor.

The models were fit to the training data and their metrics were evaluated.

```{r tab12-model cross-validation metrics}

DT::datatable(tab_model_cv_metrics)

```

The root-mean squared error (RMSE) and r^2^ indicate that the random forest model performed the best during cross-validation and thus was selected for hyperparameter tuning.

### C. Hyperparameter tuning and model selection

Given computational limitations, only two of three parameters were tuned--*trees* and *min_n*--using five levels each in a regular grid (Table 13).

```{r tab13-random forest tuning grid}

DT::datatable(grid_rf)

```

The random forest model was tuned using these hyperparameters on the cross-validation folds.

```{r fig11-random forest tuning metrics}

fig_metric_tune_rf

```

```{r tab14-random forest tuning metrics}

tab_cv_metrics_tune_rf

```

These results (Fig. 11; Table 14) indicate that the random forest model with *trees* = 2000 and *min_n* = 2 performed the best in cross-validation.

### E. Model diagnostics

Variable importance was assessed for the selected model.

```{r tab15-random forest final model vi}

DT::datatable(tab_vi_rf_final_model)

```

```{r fig12-random forest final model vip}

fig_vip_rf_final_model

```

These results indicate that *overall_qual* was the most important variable with an importance value of 1.09 x 10^12^ followed by *gr_liv_area* and *garage_cars*. Here is a boxplot showing the clear relationship between overall quality (*overall_qual*) and sale price.

```{r fig13-boxplot overall_qual sale_price}

fig_overall_qual_sale_price_box 

```

## VII. Prepare Test Data

The test data were prepared in the same manner and using the same methods as the training data (except for creating cross-validation folds).

### A. Initial data cleaning

The same data cleaning steps used on the training data were also applied to the test dataset: 1) converting variable names from camel case to snake case, 2) *garage_yr_blt* was populated with -1000 if the value was not applicable and NA if missing, 3) all character variables aside from *id* were converted to factors, and 4) not applicable values of factors were converted to variable-specific classes (e.g., "NG" for "no garage").

### B. Missingness: assessment and imputation

Missingness in the test data was assessed visually.

```{r tab16-test data missingness}

DT::datatable(tab_test_missing)

```

Results (Table 16) show that like the training data, *lot_frontage* had the most missing values (15.5%), followed by *mas_vnr_type* and *mas_vnr_area* (at roughly 1% each) and finally 16 other variables had fewer than 0.3% missing values.

Missing values were imputed using a classification and regression tree.

```{r tab17-test data imputation results}

DT::datatable(tab_summ_num_imp)

```

Note that two *utilities* values remained missing (Table 17), but this variable would subsequently be dropped due to low variance.

### C. Feature selection: drop low 'variance' and highly correlated features

Like the training data, low-variance (i..e, *street*, *utilities*, *condition2*, *pool_qc*, *roof_matl*) and highly correlated (i.e., *overall_cond*, *ms_sub_class*, *exterior2nd*) features were removed from the test data set.

### D. Feature engineering: rare-label encoding and feature scaling

Rare feature classes from the test data were binned in the same manner as the training data (Tables 9 & 10) and numerical features were normalized.

Now that the test data has been prepared in the same manner as the training data, model evaluation can begin.

## VIII. Model Evaluation

### A. Predict sale prices

Using the finalized random forest model with *trees* = 2000 and *min_n* = 2, the model was evaluated on the test data. The *sales_price* values were predicted for each row of the test data.

### B. Quantify performance on test data

The predicted *sales_price* values associated with each *id* were submitted to Kaggle for scoring. This model scored **0.16399**, which is a measure of the RMSE on logged predicted and observed sales prices.

## 
